<section id="abstract" class="abstract-box">
    <h2>Abstract</h2>
    <p>
        Continuous Variational Autoencoders (VAEs) serve as the fundamental continuous tokenizer for modern neural audio generation systems, enabling high-fidelity reconstruction while providing a compact, smooth latent space for downstream generative priors. However, continuous VAEs face a fundamental conflict when balancing <em>compression rate</em>, <em>reconstruction fidelity</em>, and <em>latent space topology</em>â€”a challenge we formalize as the <strong>Rate-Distortion-Regularity Trilemma</strong>. 
        This trilemma stems from a critical <em>topological mismatch</em>: the prevailing isotropic Gaussian prior in standard VAEs imposes a <em>flat</em> latent geometry that fails to accommodate audio's <em>hierarchical</em> nature, where low-frequency components are structured and compressible while high-frequency components are stochastic and incompressible, leading to <strong>disordered information packing</strong> where crucial semantic features are randomly interleaved with high-entropy noise. 
        To resolve this challenge, we propose <strong>Structured Topology-Aware Regularization (STAR)</strong>, a general training strategy that reshapes latent space geometry by imposing a growth-based constraint field, routing structural and textural information into channel subspaces with matching capacities. STAR is applicable to any VAE architecture and effectively resolves the trilemma, as demonstrated in CNN-based VAEs. To fully exploit STAR's potential, we present <strong>STAR-VAE</strong>, combining STAR with a hybrid CNN-Mamba architecture that synergizes local feature extraction with linear-complexity global context modeling, achieving state-of-the-art performance. We further propose <strong>STAR-Gen</strong>, an LLM-based Flow Matching framework that leverages STAR-VAE's structured latent space for high-fidelity generation without suffering from vector quantization artifacts. 
        Empirical results demonstrate that STAR-VAE successfully resolves the trilemma, achieving state-of-the-art reconstruction fidelity and enhanced semantic information preservation across diverse audio domains. The structured latent space improves both traditional diffusion models and our <strong>STAR-Gen</strong> paradigm, achieving state-of-the-art performance in text-to-audio generation.
    </p>
</section>